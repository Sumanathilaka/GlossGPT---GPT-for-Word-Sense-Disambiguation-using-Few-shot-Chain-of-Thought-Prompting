{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an apporach to predict and verify the WSD based on the sense tags.\n",
    "- GPT 4 Turbo is used as the parent model for prediction, while same model is used to verify the sense tags.\n",
    "- verification is based on Zero shot prompting technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps to follow\n",
    "\n",
    "1. Identify the senses/glosses based on the POS tag and the synonyms passsed through the query.\n",
    "2. Iterative mechanism is used to identify the sense/gloss based on the approach\n",
    "3. Zer Shot prompting is used to verify the sense/gloss based on the approach\n",
    "4. If the verification is same then the sense/gloss is identified else the sense/gloss will iterate the predicting process by reducing the definition space by 1.\n",
    "4. Process continues until the sense/gloss is identified. if the sense/gloss is not identified then the sense/gloss will be identified as \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, re\n",
    "import json\n",
    "from openai import OpenAI, ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = credentials['OPENAI_API_KEY']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "                                            model = 'gpt-4-0125-preview',\n",
    "                                            messages = [\n",
    "                                                        {\"role\": \"system\", \"content\" : \"You are a helpful assitant to identify the tag for sense of a word. You are expect to return only the requested output in the given format.\"},\n",
    "                                                        {\"role\": \"user\", \"content\": USER_MESSAGE}              \n",
    "                                                        ],\n",
    "                                            temperature=0,\n",
    "                                            max_tokens=1500\n",
    "                                            )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sense tags in FEWS dataset  663730\n",
      "['dictionary.noun.0', 'dictionary', \"A reference work with a list of words from one or more languages, normally ordered alphabetically, explaining each word's meaning, and sometimes containing information on its etymology, pronunciation, usage, translations, and other data.\", '', '1', 'wordbook']\n"
     ]
    }
   ],
   "source": [
    "# Read data from file\n",
    "with open('senses.txt', 'r',encoding=\"utf8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Split the data into entries based on empty lines\n",
    "entries = data.strip().split('\\n\\n')\n",
    "print(\"No of sense tags in FEWS dataset \",len(entries))\n",
    "\n",
    "# Create a list of lists for each entry's details\n",
    "list_of_lists = []\n",
    "for entry in entries:\n",
    "    details = entry.split('\\n')\n",
    "    entry_list = []\n",
    "    for detail in details:\n",
    "        _, value = detail.split(':', 1)\n",
    "        entry_list.append(value.strip())\n",
    "    list_of_lists.append(entry_list)\n",
    "\n",
    "# Print the instance from the sense tag\n",
    "print(list_of_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the word into the base word as the sense are kept in base\n",
    "def word_base(word):\n",
    "    try:\n",
    "        word=stemmer.stem(word)\n",
    "        base_word = lemmatizer.lemmatize(word)\n",
    "        return base_word\n",
    "    except:\n",
    "        return word   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to retrieve the word meaning from the list_of_list list\n",
    "#this function will specifically read the sense id and the meaning(gloss) which is required for the processing.\n",
    "def retrieve_meanings(word, data):\n",
    "    meanings_dict = {}\n",
    "    for entry in data:\n",
    "        if word == entry[0].split(\".\")[0]:\n",
    "            if entry[-1] !=\"\":\n",
    "                meanings_dict[entry[0]] = entry[2]+\", synonyms :\"+entry[-1]\n",
    "            else:\n",
    "                meanings_dict[entry[0]] = entry[2]\n",
    "    return meanings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the sentence without <wsd> token and the index\n",
    "def extract_word_and_index(sentence):\n",
    "    # Find the start and end index of the <WSD> tags\n",
    "    start_index = sentence.find('<WSD>')    \n",
    "    end_index = sentence.find('</WSD>')\n",
    "    \n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract the word between <WSD> tags\n",
    "        word = sentence[start_index + len('<WSD>'):end_index]\n",
    "\n",
    "        # Remove <WSD> and </WSD> tags from the sentence\n",
    "        cleaned_sentence = sentence[:start_index] + word+\" \" + sentence[end_index + len('</WSD>'):]\n",
    "            \n",
    "        return cleaned_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to identify the WSD word from the given sentence and return the WSD word on a sentence\n",
    "def wsdword(text):\n",
    "    match = re.search(r'<WSD>(.*?)</WSD>', text)\n",
    "    if match:\n",
    "        word_inside_wsd = match.group(1)\n",
    "        return word_inside_wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to counts the number of tokens : input and output tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Count the tokens in the input\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\2358452\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Word lemmatizer to get the base word of the WSD word\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_dictionary_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data_dict = json.load(file)\n",
    "        return data_dict\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the file doesn't exist\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle the case where the file contains invalid JSON\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postag_preprocess(pos_tag):\n",
    "    if pos_tag==\"adj\":\n",
    "        return \"adjective\"\n",
    "    elif pos_tag==\"noun\" or pos_tag==\"verb\":\n",
    "        return pos_tag\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = load_dictionary_from_file(\"my_dictionary.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_elements_by_key(meaning_list, key_to_remove):\n",
    "    # Find indices of elements with matching keys\n",
    "    indices_to_remove = [i for i, (key, _) in enumerate(meaning_list) if key == key_to_remove]\n",
    "\n",
    "    # Remove elements from the list in reverse order to avoid index errors\n",
    "    for index in sorted(indices_to_remove, reverse=True):\n",
    "        meaning_list.pop(index)\n",
    "    return meaning_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json(output):\n",
    "    # Remove triple quotes and leading/trailing whitespace\n",
    "    output = output.strip()[7:-3].strip()\n",
    "\n",
    "    # Convert to JSON format\n",
    "    json_data = json.loads(output)\n",
    "\n",
    "    # Convert back to a JSON string with indentation for readability\n",
    "    formatted_json = json.dumps(json_data, indent=4)\n",
    "\n",
    "    return formatted_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense_Tag_Return_pipeline(sentence,postag,wordwsd):\n",
    "    \n",
    "        ob=\"{\"\n",
    "        cb=\"}\"\n",
    "        \n",
    "        meanings = retrieve_meanings(wordwsd, list_of_lists)\n",
    "        #print(meanings)\n",
    "        cleaned_sentence= extract_word_and_index(sentence) \n",
    "        \n",
    "        #retrieving the data form the dictionary\n",
    "        filtered_definitions = {key: value for key, value in meanings.items() if postag in key}\n",
    "        #print(filtered_definitions)\n",
    "        meaning_list= list(filtered_definitions.items())      \n",
    "        #print(meaning_list)\n",
    "        count=0 #variable to count the tokens \n",
    "        \n",
    "        if wsdword(sentence) in loaded_dict:\n",
    "            #print(\"examples found\")\n",
    "            examples=loaded_dict[wsdword(sentence)]\n",
    "            try:\n",
    "                examples=examples[postag]\n",
    "            except:\n",
    "                examples=examples\n",
    "            \n",
    "\n",
    "        elif word_base(wsdword(sentence)) in loaded_dict:\n",
    "            #print(\"examples found\")\n",
    "            examples=loaded_dict[word_base(wsdword(sentence))]\n",
    "            try:\n",
    "                examples=examples[postag]\n",
    "            except:\n",
    "                examples=examples\n",
    "        else:\n",
    "            examples=None\n",
    "            \n",
    "        #print(examples)\n",
    "        \n",
    "        \n",
    "        count=0 #variable to count the tokens \n",
    "        flag=0\n",
    "        for count_meaning in range(0,len(meaning_list)):\n",
    "            filtered_examples=[]\n",
    "            if(examples != None):\n",
    "                for example in examples:\n",
    "                    for meaning in meaning_list:\n",
    "                        if meaning[0] in example:\n",
    "                            filtered_examples.append(example)\n",
    "                    \n",
    "            #print(filtered_examples)                   \n",
    "            prompt_predict = f'''You are going to identify the corresponding sense tag of an ambiguous word in English sentences. Use multiple reasoning strategies to increase confidence in your answer.\n",
    "1. The word \"{wordwsd}\" has different meanings. Below are possible meanings. Comprehend the sense tags and meanings. Synonyms are provided if available. {meaning_list}\n",
    "2. You can learn more on the usage of each word and the its sense through the examples below. Each sentence is followed by its corresponding sense id. \"{filtered_examples}\"\n",
    "3. Now carefully examine the sentence below. The ambiguous word is enclosed within <WSD>.\"{sentence}\"\n",
    "4. Analyze the sentence using the following techniques and identify the meaning of the ambiguous word.\n",
    "   Focus on keywords in the sentence surrounding the ambiguous word. \n",
    "   Think about the overall topic and intent of the sentence. Decide on the sense of the word that makes the most logical sense within the context. \n",
    "5. Based on the identified meaning, try to find the most appropriate senseID from the below sense tag list. You are given definition of each sense tag too.\"{meaning_list}\".\n",
    "6. Return only the identified senseid in the below JSON format. Do not add any extra information\n",
    "    {ob}\"sense_id\":  the identified sense id{cb} '''\n",
    "            \n",
    "            count+=count_tokens(prompt_predict)\n",
    "            response=complete_model(prompt_predict)\n",
    "            #print(response)\n",
    "            try:\n",
    "                data = json.loads(response)\n",
    "            except:\n",
    "                print(response)\n",
    "                standard_json = convert_to_json(response)\n",
    "                data = json.loads(standard_json)\n",
    "                \n",
    "\n",
    "\n",
    "            value = data['sense_id']\n",
    "            try:\n",
    "                identified_meaning=filtered_definitions[value]\n",
    "            except:\n",
    "                continue\n",
    "            #print(identified_meaning)\n",
    "\n",
    "            prompt_verify=f'''You are going to verify the corresponding sense tag of an ambiguous word in English sentences. Use multiple reasoning strategies to increase confidence in your answer.\n",
    "1  Carefully examine the sentence below. The ambiguous word is enclosed within <WSD>.\"{sentence}\"\n",
    "2. Analyze the sentence using the following techniques and identify the meaning of the ambiguous word.\n",
    "   Focus on keywords in the sentence surrounding the ambiguous word. \n",
    "   Think about the overall topic and intent of the sentence.\n",
    "3. Verify whether the meaning you identified is same as below meaning or not. {identified_meaning}\n",
    "4. If same return TRUE else Return FALSE\n",
    "4. Return only the TRUE or FALSE in the below JSON format.\n",
    "    {ob}\"Response\":  TRUE/FALSE{cb} '''\n",
    "            \n",
    "            count+=count_tokens(prompt_verify)\n",
    "            response_verify=complete_model(prompt_verify)\n",
    "            #print(response_verify)\n",
    "\n",
    "            if (\"TRUE\" in response_verify):\n",
    "                flag=1\n",
    "                #print(response)\n",
    "                return value,count\n",
    "            else:\n",
    "                meaning_list=remove_elements_by_key(meaning_list,value)\n",
    "                #print(meaning_list)\n",
    "            \n",
    "        if flag==0:\n",
    "            return None,count     \n",
    "       \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beast.noun.0\n",
      "brake.noun.1\n",
      "acquirement.noun.1\n",
      "stage.noun.11\n",
      "part.noun.11\n",
      "red_shirt.noun.0\n",
      "people_pod.noun.4\n",
      "None\n",
      "corruption.noun.0\n",
      "palaver.noun.3\n",
      "interest.noun.3\n",
      "concretization.noun.1\n",
      "bourne.noun.0\n",
      "bizarreness.noun.1\n",
      "None\n",
      "farming.noun.0\n",
      "pique.noun.0\n",
      "naturalism.noun.0\n",
      "pigstick.noun.3\n",
      "biopunk.noun.0\n",
      "stream.noun.0\n",
      "patmosian.noun.1\n",
      "mean.noun.0\n",
      "None\n",
      "sketch.noun.1\n",
      "loo.noun.7\n",
      "none.noun.0\n",
      "odd_job.noun.0\n",
      "disciplinist.noun.0\n",
      "itchy_trigger_finger.noun.1\n",
      "wight.noun.0\n",
      "nature.noun.1\n",
      "wyrd.noun.0\n",
      "langlauf.noun.1\n",
      "None\n",
      "wet_sanding.noun.1\n",
      "castellany.noun.1\n",
      "people_pod.noun.1\n",
      "matron.noun.0\n",
      "incivility.noun.1\n",
      "arrival.noun.0\n",
      "None\n",
      "expediency.noun.1\n",
      "interferant.noun.1\n",
      "knightdom.noun.1\n",
      "turn_of_foot.noun.2\n",
      "vanishing_point.noun.1\n",
      "ditto.noun.2\n",
      "subvention.noun.0\n",
      "needle_dick.noun.0\n",
      "rapefic.noun.0\n",
      "attack.noun.2\n",
      "witness.noun.4\n",
      "case.noun.0\n",
      "None\n",
      "ologun.noun.1\n",
      "None\n",
      "zephyr.verb.0\n",
      "touch.verb.21\n",
      "afterthink.verb.0\n",
      "kiss.verb.2\n",
      "adventure.verb.0\n",
      "None\n",
      "dismay.verb.0\n",
      "revirginate.verb.1\n",
      "thrill.verb.1\n",
      "draw_a_line_in_the_sand.verb.2\n",
      "tighten_up.verb.2\n",
      "gorge.verb.0\n",
      "underwrap.verb.1\n",
      "concentre.verb.3\n",
      "uncowl.verb.2\n",
      "applaud.verb.0\n",
      "behedge.verb.3\n",
      "run.verb.52\n",
      "overamplify.verb.1\n",
      "overplate.verb.2\n",
      "disembogue.verb.0\n",
      "object.verb.0\n",
      "address.verb.7\n",
      "rankle.verb.1\n",
      "mop_up.verb.6\n",
      "None\n",
      "None\n",
      "stickle.verb.1\n",
      "overween.verb.0\n",
      "will.verb.3\n",
      "stay.verb.16\n",
      "jackhammer.verb.3\n",
      "encroach.verb.2\n",
      "wither.verb.1\n",
      "None\n",
      "fill.verb.3\n",
      "close_the_book_on.verb.1\n",
      "wheel.verb.2\n",
      "overpoise.verb.2\n",
      "twine.verb.2\n",
      "None\n",
      "suck_all_the_air_out_of.verb.0\n",
      "joy.verb.2\n",
      "recover.verb.1\n",
      "overset.verb.5\n",
      "come_together.verb.2\n",
      "None\n",
      "None\n",
      "try.verb.0\n",
      "conduct.verb.5\n",
      "fettle.verb.1\n",
      "None\n",
      "bristle.verb.0\n",
      "go_nuclear.verb.3\n",
      "jackhammer.verb.3\n",
      "None\n",
      "wish.verb.0\n",
      "undercode.verb.0\n",
      "wake_up.verb.1\n",
      "let_loose.verb.1\n",
      "octavate.verb.2\n",
      "None\n",
      "blazon.verb.3\n",
      "shore.verb.2\n",
      "unstring.verb.4\n",
      "bename.verb.1\n",
      "stay.verb.3\n",
      "drape.verb.1\n",
      "peach.verb.0\n",
      "sciencey.adjective.1\n",
      "light-handed.adjective.2\n",
      "underwrapped.adjective.2\n",
      "speculative.adjective.1\n",
      "placable.adjective.0\n",
      "unpretended.adjective.0\n",
      "funky.adjective.0\n",
      "lunar.adjective.4\n",
      "None\n",
      "hackerish.adjective.0\n",
      "punctilious.adjective.0\n",
      "juvenile.adjective.0\n",
      "monsterful.adjective.1\n",
      "bolt-on.adjective.1\n",
      "memoriter.adjective.1\n",
      "drooly.adjective.1\n",
      "troublous.adjective.2\n",
      "incessive.adjective.2\n",
      "schooly.adjective.0\n",
      "chasmous.adjective.0\n",
      "gluttonous.adjective.1\n",
      "None\n",
      "off_the_grid.adjective.2\n",
      "livid.adjective.1\n",
      "None\n",
      "unwearied.adjective.1\n",
      "warm.adjective.1\n",
      "tender-handed.adjective.0\n",
      "continent.adjective.0\n",
      "None\n",
      "dog_rough.adjective.1\n",
      "less.adjective.1\n",
      "None\n",
      "sinister.adjective.1\n",
      "close.adjective.17\n",
      "bitter.adjective.3\n",
      "unhusked.adjective.1\n",
      "None\n",
      "ambitious.adjective.2\n",
      "fatty.adjective.0\n",
      "profligate.adjective.0\n",
      "decent.adjective.0\n",
      "main.adjective.0\n",
      "superdominant.adjective.2\n",
      "zombic.adjective.1\n",
      "ithyphallic.adjective.3\n",
      "plutonian.adjective.1\n",
      "sere.adjective.0\n",
      "cultureful.adjective.0\n",
      "achillean.adjective.2\n",
      "disparate.adjective.0\n",
      "None\n",
      "cousinfucking.adjective.1\n",
      "demonifugic.adjective.1\n",
      "about.adverb.2\n",
      "sennight.adverb.0\n",
      "scarcely.adverb.2\n",
      "back.adverb.4\n",
      "seasonably.adverb.2\n",
      "together.adverb.0\n",
      "affectedly.adverb.0\n",
      "scarcely.adverb.2\n",
      "None\n",
      "None\n",
      "first_thing.adverb.1\n",
      "318764\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "#evaluating the results\n",
    "filename=\"missing_data_analysis_4/predictnverifyv3\"+datetime.date.today().strftime(\"%B %d, %Y\")+\".txt\"\n",
    "file=open(\"missing_data_analysis_4/missing_test_data.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "file2=open(filename,\"w\",encoding=\"utf8\")\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    lst=i.split(\"\t\")\n",
    "    sentence,senseid=lst[0],lst[1]\n",
    "    postag=senseid.split(\".\")[1]  #postag \n",
    "    wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "    #print(postag)\n",
    "    foutput,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "    totalTokens+=token_count\n",
    "    #print(output)\n",
    "    print(foutput)\n",
    "    #writing the output in the file\n",
    "    file2.write(str(foutput) + \"\\n\")\n",
    "    \n",
    "\n",
    "file.close()\n",
    "file2.close()\n",
    "print(totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beast.noun.0\n",
      "brake.noun.1\n",
      "acquirement.noun.1\n",
      "stage.noun.11\n",
      "part.noun.9\n",
      "red_shirt.noun.0\n",
      "people_pod.noun.4\n",
      "None\n",
      "corruption.noun.0\n",
      "palaver.noun.3\n",
      "interest.noun.3\n",
      "concretization.noun.1\n",
      "bourne.noun.0\n",
      "None\n",
      "None\n",
      "farming.noun.0\n",
      "pique.noun.1\n",
      "naturalism.noun.0\n",
      "pigstick.noun.3\n",
      "biopunk.noun.0\n",
      "stream.noun.0\n",
      "patmosian.noun.1\n",
      "mean.noun.0\n",
      "None\n",
      "sketch.noun.1\n",
      "loo.noun.7\n",
      "none.noun.0\n",
      "None\n",
      "disciplinist.noun.0\n",
      "itchy_trigger_finger.noun.1\n",
      "wight.noun.2\n",
      "None\n",
      "wyrd.noun.0\n",
      "None\n",
      "None\n",
      "wet_sanding.noun.0\n",
      "None\n",
      "people_pod.noun.1\n",
      "matron.noun.0\n",
      "incivility.noun.1\n",
      "arrival.noun.0\n",
      "None\n",
      "expediency.noun.1\n",
      "interferant.noun.1\n",
      "knightdom.noun.1\n",
      "turn_of_foot.noun.2\n",
      "vanishing_point.noun.1\n",
      "ditto.noun.2\n",
      "subvention.noun.0\n",
      "needle_dick.noun.0\n",
      "rapefic.noun.0\n",
      "attack.noun.2\n",
      "witness.noun.4\n",
      "case.noun.0\n",
      "dope_sheet.noun.0\n",
      "None\n",
      "None\n",
      "zephyr.verb.0\n",
      "touch.verb.21\n",
      "None\n",
      "kiss.verb.0\n",
      "adventure.verb.0\n",
      "None\n",
      "dismay.verb.0\n",
      "revirginate.verb.1\n",
      "None\n",
      "draw_a_line_in_the_sand.verb.2\n",
      "tighten_up.verb.4\n",
      "gorge.verb.0\n",
      "underwrap.verb.1\n",
      "concentre.verb.3\n",
      "uncowl.verb.2\n",
      "None\n",
      "behedge.verb.3\n",
      "run.verb.52\n",
      "None\n",
      "overplate.verb.2\n",
      "disembogue.verb.0\n",
      "object.verb.0\n",
      "address.verb.7\n",
      "rankle.verb.1\n",
      "mop_up.verb.6\n",
      "None\n",
      "None\n",
      "stickle.verb.1\n",
      "overween.verb.0\n",
      "will.verb.3\n",
      "stay.verb.16\n",
      "jackhammer.verb.6\n",
      "encroach.verb.2\n",
      "wither.verb.0\n",
      "None\n",
      "fill.verb.2\n",
      "close_the_book_on.verb.1\n",
      "wheel.verb.2\n",
      "overpoise.verb.1\n",
      "twine.verb.2\n",
      "furnish.verb.1\n",
      "suck_all_the_air_out_of.verb.0\n",
      "joy.verb.2\n",
      "recover.verb.1\n",
      "overset.verb.5\n",
      "come_together.verb.2\n",
      "apprehend.verb.2\n",
      "None\n",
      "try.verb.0\n",
      "conduct.verb.3\n",
      "fettle.verb.1\n",
      "None\n",
      "bristle.verb.0\n",
      "go_nuclear.verb.3\n",
      "jackhammer.verb.3\n",
      "None\n",
      "None\n",
      "undercode.verb.0\n",
      "wake_up.verb.1\n",
      "let_loose.verb.1\n",
      "octavate.verb.2\n",
      "None\n",
      "blazon.verb.3\n",
      "shore.verb.2\n",
      "unstring.verb.2\n",
      "bename.verb.2\n",
      "stay.verb.3\n",
      "drape.verb.1\n",
      "peach.verb.0\n",
      "sciencey.adjective.1\n",
      "light-handed.adjective.2\n",
      "underwrapped.adjective.2\n",
      "None\n",
      "placable.adjective.0\n",
      "unpretended.adjective.0\n",
      "funky.adjective.0\n",
      "lunar.adjective.0\n",
      "None\n",
      "hackerish.adjective.0\n",
      "punctilious.adjective.0\n",
      "None\n",
      "monsterful.adjective.1\n",
      "bolt-on.adjective.1\n",
      "memoriter.adjective.1\n",
      "None\n",
      "troublous.adjective.2\n",
      "incessive.adjective.2\n",
      "schooly.adjective.0\n",
      "chasmous.adjective.0\n",
      "None\n",
      "None\n",
      "off_the_grid.adjective.2\n",
      "livid.adjective.0\n",
      "None\n",
      "unwearied.adjective.1\n",
      "warm.adjective.1\n",
      "None\n",
      "continent.adjective.0\n",
      "None\n",
      "dog_rough.adjective.1\n",
      "less.adjective.1\n",
      "None\n",
      "sinister.adjective.1\n",
      "None\n",
      "bitter.adjective.3\n",
      "unhusked.adjective.1\n",
      "temperate.adjective.3\n",
      "ambitious.adjective.2\n",
      "fatty.adjective.0\n",
      "None\n",
      "decent.adjective.0\n",
      "main.adjective.0\n",
      "superdominant.adjective.1\n",
      "zombic.adjective.1\n",
      "ithyphallic.adjective.3\n",
      "plutonian.adjective.1\n",
      "sere.adjective.0\n",
      "cultureful.adjective.1\n",
      "achillean.adjective.2\n",
      "disparate.adjective.0\n",
      "None\n",
      "cousinfucking.adjective.1\n",
      "demonifugic.adjective.1\n",
      "about.adverb.2\n",
      "None\n",
      "scarcely.adverb.2\n",
      "back.adverb.4\n",
      "seasonably.adverb.2\n",
      "together.adverb.0\n",
      "affectedly.adverb.0\n",
      "None\n",
      "None\n",
      "None\n",
      "first_thing.adverb.1\n",
      "337155\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "#evaluating the results\n",
    "filename=\"missing_data_analysis_4/predictnverifyv2\"+datetime.date.today().strftime(\"%B %d, %Y\")+\".txt\"\n",
    "file=open(\"missing_data_analysis_4/missing_test_data.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "file2=open(filename,\"w\",encoding=\"utf8\")\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    lst=i.split(\"\t\")\n",
    "    sentence,senseid=lst[0],lst[1]\n",
    "    postag=senseid.split(\".\")[1]  #postag \n",
    "    wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "    #print(postag)\n",
    "    foutput,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "    totalTokens+=token_count\n",
    "    #print(output)\n",
    "    print(foutput)\n",
    "    #writing the output in the file\n",
    "    file2.write(str(foutput) + \"\\n\")\n",
    "    \n",
    "\n",
    "file.close()\n",
    "file2.close()\n",
    "print(totalTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 47 instances are corrected using this apporach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
